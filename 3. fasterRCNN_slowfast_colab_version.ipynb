{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"fasterRCNN_slowfast.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7f0082178b2d49068e5848d5917acd24":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1418b30ac5244afa970b218ee9008c2e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7ef029921c8f40068b141dfec0d4ba6f","IPY_MODEL_a4fea320836d47a79b96d5a952d201b5","IPY_MODEL_763207c78902458885c4e6fb262dcd07"]}},"1418b30ac5244afa970b218ee9008c2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ef029921c8f40068b141dfec0d4ba6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3e8827e5360f49cb9649a58f6d3345e4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e2f5df9289340039f91a3f782b5f8f6"}},"a4fea320836d47a79b96d5a952d201b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9487b7f021504a42b397ff29d6b310b5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":271252349,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":271252349,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d93079e7624d48e1b3f0d63c2f691850"}},"763207c78902458885c4e6fb262dcd07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e010b52a2892495f86fd84beff05f5f1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 259M/259M [00:09&lt;00:00, 31.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_885e672329bf40de96254187906d8bb9"}},"3e8827e5360f49cb9649a58f6d3345e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5e2f5df9289340039f91a3f782b5f8f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9487b7f021504a42b397ff29d6b310b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d93079e7624d48e1b3f0d63c2f691850":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e010b52a2892495f86fd84beff05f5f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"885e672329bf40de96254187906d8bb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"efb44e9e"},"source":["# ref https://github.com/facebookresearch/pytorchvideo/blob/master/tutorials/video_detection_example/video_detection_inference_tutorial.ipynb"],"id":"efb44e9e"},{"cell_type":"markdown","metadata":{"id":"cI2ENLb49Jit"},"source":["download zip to cloud hard drive \n","https://github.com/facebookresearch/detectron2"],"id":"cI2ENLb49Jit"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2RisUvBZZZbi","executionInfo":{"status":"ok","timestamp":1630483825819,"user_tz":-480,"elapsed":185655,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"d16b52ff-feb3-41ae-d11f-d9b06aa2f1ee"},"source":["!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"],"id":"2RisUvBZZZbi","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/facebookresearch/detectron2.git\n","  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-ciyq1oh1\n","  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-ciyq1oh1\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (3.2.2)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (2.0.2)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (1.1.0)\n","Collecting yacs>=0.1.6\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (0.8.9)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (1.3.0)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (4.62.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (2.6.0)\n","Collecting fvcore<0.1.6,>=0.1.5\n","  Downloading fvcore-0.1.5.post20210825.tar.gz (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 2.8 MB/s \n","\u001b[?25hCollecting iopath<0.1.10,>=0.1.7\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (0.16.0)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.5) (1.3.0)\n","Collecting omegaconf>=2.1\n","  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.2 MB/s \n","\u001b[?25hCollecting hydra-core>=1.1\n","  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 40.5 MB/s \n","\u001b[?25hCollecting black==21.4b2\n","  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n","\u001b[K     |████████████████████████████████| 130 kB 46.1 MB/s \n","\u001b[?25hCollecting regex>=2020.1.8\n","  Downloading regex-2021.8.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (745 kB)\n","\u001b[K     |████████████████████████████████| 745 kB 43.4 MB/s \n","\u001b[?25hCollecting pathspec<1,>=0.8.1\n","  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.5) (3.7.4.3)\n","Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.5) (0.10.2)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.5) (1.4.4)\n","Collecting mypy-extensions>=0.4.3\n","  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n","Collecting typed-ast>=1.4.2\n","  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n","\u001b[K     |████████████████████████████████| 743 kB 36.4 MB/s \n","\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.5) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.5) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 44.5 MB/s \n","\u001b[?25hCollecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 50.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.5) (5.2.2)\n","Collecting portalocker\n","  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.5) (57.4.0)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.5) (0.29.24)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.5) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.5) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.5) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.5) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->detectron2==0.5) (1.15.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.5) (3.5.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (0.37.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (1.34.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (3.3.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (0.4.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (2.23.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (3.17.3)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (1.39.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (0.12.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.5) (1.8.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.5) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.5) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.5) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.5) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.5) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2==0.5) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.5) (2021.5.30)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.5) (3.1.1)\n","Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n","  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for detectron2: filename=detectron2-0.5-cp37-cp37m-linux_x86_64.whl size=5518411 sha256=8b366fb61c63e2e0cb1417a831f4970b3c78a58ae71761c367707db48ced18e6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-sft1dgbc/wheels/07/dc/32/0322cb484dbefab8b9366bfedbaff5060ac7d149d69c27ca5d\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20210825-py3-none-any.whl size=60661 sha256=f3b83e77747ff4167f5c2d288c684e54756a86be9814bf051c4572ca809dd0e1\n","  Stored in directory: /root/.cache/pip/wheels/53/c4/f8/c4cb07f135845218b019b4a55d8a0470a0f21ee13f8dcd16be\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=4b969a6084f176c0dede08ccd033dc2904f245975a28187dc8efcf072f275d7f\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","Successfully built detectron2 fvcore antlr4-python3-runtime\n","Installing collected packages: pyyaml, portalocker, antlr4-python3-runtime, yacs, typed-ast, regex, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","Successfully installed antlr4-python3-runtime-4.8 black-21.4b2 detectron2-0.5 fvcore-0.1.5.post20210825 hydra-core-1.1.1 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.1.1 pathspec-0.9.0 portalocker-2.3.2 pyyaml-5.4.1 regex-2021.8.28 typed-ast-1.4.3 yacs-0.1.8\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQkX8GwVwca4","executionInfo":{"status":"ok","timestamp":1630483868570,"user_tz":-480,"elapsed":42757,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"b4199f49-3df8-4b06-f5ef-29e32d15322c"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)"],"id":"JQkX8GwVwca4","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTabpOFTzVea","executionInfo":{"status":"ok","timestamp":1630483868960,"user_tz":-480,"elapsed":402,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"9b54322a-5bda-4389-81af-62ae07fcec29"},"source":["%cd /content/gdrive/MyDrive/slowfast\n","%ls"],"id":"eTabpOFTzVea","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/slowfast\n","Anaconda3-5.2.0-Linux-x86_64.sh  IMG_2291.MOV\n","ava_action_list.txt              IMG_4515.MOV\n","ckpt.t7                          \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n","darknet.py                       slowfast_colab.ipynb\n","DeepSort_and_KeyPoints.ipynb     slowfast_HIIT.avi\n","deep_sort_pytorch-master.zip     Slow-Fast-pytorch-implementation-master.zip\n","deepsort_slowfast.ipynb          slowfast_weight.pth\n","detectron2-master.zip            visualization.py\n","fasterRCNN_slowfast.ipynb        workout.mp4\n","\u001b[01;34mframes\u001b[0m/                          yolov3.cfg\n","IMG_2291.avi                     yolov3.weights\n"]}]},{"cell_type":"code","metadata":{"id":"GK4eKp1tzVhq","executionInfo":{"status":"ok","timestamp":1630483884135,"user_tz":-480,"elapsed":3492,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}}},"source":["!python visualization.py"],"id":"GK4eKp1tzVhq","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Weg27DQZxdP3","executionInfo":{"status":"ok","timestamp":1630483894192,"user_tz":-480,"elapsed":10061,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"0e0e9299-471b-4f95-e401-ce4157c3fa79"},"source":["!pip install pytorchvideo"],"id":"Weg27DQZxdP3","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorchvideo\n","  Downloading pytorchvideo-0.1.2.tar.gz (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: fvcore in /usr/local/lib/python3.7/dist-packages (from pytorchvideo) (0.1.5.post20210825)\n","Collecting av\n","  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\n","\u001b[K     |████████████████████████████████| 37.2 MB 29 kB/s \n","\u001b[?25hCollecting parameterized\n","  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: iopath in /usr/local/lib/python3.7/dist-packages (from pytorchvideo) (0.1.9)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (4.62.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (5.4.1)\n","Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.1.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.19.5)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.1.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (7.1.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.8.9)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath->pytorchvideo) (2.3.2)\n","Building wheels for collected packages: pytorchvideo\n","  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.2-py3-none-any.whl size=166136 sha256=67ae8227e320775dd45703fd5c8b5e9aa8a9720d98d07aa403348765ac5c686f\n","  Stored in directory: /root/.cache/pip/wheels/4c/f5/ee/ee524d0e7df398623c64816181cd6a99fb9b7eab56d0608ace\n","Successfully built pytorchvideo\n","Installing collected packages: parameterized, av, pytorchvideo\n","Successfully installed av-8.0.3 parameterized-0.8.1 pytorchvideo-0.1.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6e64fd7d","executionInfo":{"status":"ok","timestamp":1630483899280,"user_tz":-480,"elapsed":1304,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"48f78cad-285c-4dc9-f30c-733ba2842ab8"},"source":["from functools import partial\n","import numpy as np\n","import imageio\n","import cv2\n","import torch\n","\n","import detectron2\n","from detectron2.config import get_cfg\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","\n","import pytorchvideo\n","from pytorchvideo.transforms.functional import (\n","    uniform_temporal_subsample,\n","    short_side_scale_with_boxes,\n","    clip_boxes_to_image,\n",")\n","from torchvision.transforms._functional_video import normalize\n","from pytorchvideo.data.ava import AvaLabeledVideoFramePaths\n","from pytorchvideo.models.hub import slow_r50_detection,slowfast_r50_detection # Another option is slowfast_r50_detection\n","\n","from visualization import VideoVisualizer"],"id":"6e64fd7d","execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.\n","  \"The _functional_video module is deprecated. Please use the functional module instead.\"\n"]}]},{"cell_type":"markdown","metadata":{"id":"F6niSCKMh7Wd"},"source":["#Load Model using Torch Hub API\n","\n","PyTorchVideo provides several pretrained models through Torch Hub. Available models are described in model zoo documentation.\n","\n","Here we are selecting the slow_r50_detection model which was trained using a 4x16 setting on the Kinetics 400 dataset and fine tuned on AVA V2.2 actions dataset.\n","\n","NOTE: to run on GPU in Google Colab, in the menu bar selet: Runtime -> Change runtime type -> Harware Accelerator -> GPU"],"id":"F6niSCKMh7Wd"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["7f0082178b2d49068e5848d5917acd24","1418b30ac5244afa970b218ee9008c2e","7ef029921c8f40068b141dfec0d4ba6f","a4fea320836d47a79b96d5a952d201b5","763207c78902458885c4e6fb262dcd07","3e8827e5360f49cb9649a58f6d3345e4","5e2f5df9289340039f91a3f782b5f8f6","9487b7f021504a42b397ff29d6b310b5","d93079e7624d48e1b3f0d63c2f691850","e010b52a2892495f86fd84beff05f5f1","885e672329bf40de96254187906d8bb9"]},"id":"ece31cc6","executionInfo":{"status":"ok","timestamp":1630483925785,"user_tz":-480,"elapsed":20936,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"99f282e8-0687-4e56-bc45-a97d542ee501"},"source":["device = 'cuda' # or 'cpu'\n","video_model = slowfast_r50_detection(True) # Another option is slowfast_r50_detection\n","#video_model = slow_r50_detection(True) # Another option is slowfast_r50_detection\n","video_model = video_model.eval().to(device)"],"id":"ece31cc6","execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/ava/SLOWFAST_8x8_R50_DETECTION.pyth\" to /root/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R50_DETECTION.pyth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f0082178b2d49068e5848d5917acd24","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/259M [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"vjBqUt_eiET6"},"source":["#Load an off-the-shelf Detectron2 object detector\n","\n","We use the object detector to detect bounding boxes for the people. These bounding boxes later feed into our video action detection model. For more details, please refer to the Detectron2's object detection tutorials.\n","\n","To install Detectron2, please follow the instructions mentioned https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md"],"id":"vjBqUt_eiET6"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c42b263f","executionInfo":{"status":"ok","timestamp":1630483935976,"user_tz":-480,"elapsed":7717,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"95b500a1-c0bb-4796-aed6-811de64df7fd"},"source":["cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.55  # set threshold for this model\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n","predictor = DefaultPredictor(cfg)\n","\n","# This method takes in an image and generates the bounding boxes for people in the image.\n","def get_person_bboxes(inp_img, predictor):\n","    predictions = predictor(inp_img.cpu().detach().numpy())['instances'].to('cpu')\n","    boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n","    scores = predictions.scores if predictions.has(\"scores\") else None\n","    classes = np.array(predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None)\n","    predicted_boxes = boxes[np.logical_and(classes==0, scores>0.75 )].tensor.cpu() # only person\n","    return predicted_boxes"],"id":"c42b263f","execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["model_final_280758.pkl: 167MB [00:06, 25.3MB/s]                           \n"]}]},{"cell_type":"markdown","metadata":{"id":"9asPyo09ifCZ"},"source":["#Define the transformations for the input required by the model\n","\n","Before passing the video and bounding boxes into the model we need to apply some input transforms and sample a clip of the correct frame rate in the clip.\n","\n","Here, below we define a method that can pre-process the clip and bounding boxes. It generates inputs accordingly for both Slow (Resnet) and SlowFast models depending on the parameterization of the variable slow_fast_alpha."],"id":"9asPyo09ifCZ"},{"cell_type":"code","metadata":{"id":"d92ab783","executionInfo":{"status":"ok","timestamp":1630483942955,"user_tz":-480,"elapsed":237,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}}},"source":["def ava_inference_transform(\n","    clip, \n","    boxes,\n","    #num_frames = 4, #if using slow_r50_detection, change this to 4\n","    num_frames = 32, #if using slowfast_r50_detection, change this to 32\n","    crop_size = 256, \n","    data_mean = [0.45, 0.45, 0.45], \n","    data_std = [0.225, 0.225, 0.225],\n","    #slow_fast_alpha = None, #if using slow_r50_detection, change this to None\n","    slow_fast_alpha = 4, #if using slowfast_r50_detection, change this to 4\n","):\n","\n","    boxes = np.array(boxes)\n","    ori_boxes = boxes.copy()\n","\n","    # Image [0, 255] -> [0, 1].\n","    clip = uniform_temporal_subsample(clip, num_frames)\n","    clip = clip.float()\n","    clip = clip / 255.0\n","\n","    height, width = clip.shape[2], clip.shape[3]\n","    # The format of boxes is [x1, y1, x2, y2]. The input boxes are in the\n","    # range of [0, width] for x and [0,height] for y\n","    boxes = clip_boxes_to_image(boxes, height, width)\n","\n","    # Resize short side to crop_size. Non-local and STRG uses 256.\n","    clip, boxes = short_side_scale_with_boxes(\n","        clip,\n","        size=crop_size,\n","        boxes=boxes,\n","    )\n","    \n","    # Normalize images by mean and std.\n","    clip = normalize(\n","        clip,\n","        np.array(data_mean, dtype=np.float32),\n","        np.array(data_std, dtype=np.float32),\n","    )\n","    \n","    boxes = clip_boxes_to_image(\n","        boxes, clip.shape[2],  clip.shape[3]\n","    )\n","    \n","    # Incase of slowfast, generate both pathways\n","    if slow_fast_alpha is not None:\n","        fast_pathway = clip\n","        # Perform temporal sampling from the fast pathway.\n","        slow_pathway = torch.index_select(\n","            clip,\n","            1,\n","            torch.linspace(\n","                0, clip.shape[1] - 1, clip.shape[1] // slow_fast_alpha\n","            ).long(),\n","        )\n","        clip = [slow_pathway, fast_pathway]\n","    \n","    return clip, torch.from_numpy(boxes), ori_boxes"],"id":"d92ab783","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7fQ685tYiqDo"},"source":["#Setup\n","Download the id to label mapping for the AVA V2.2 dataset on which the Torch Hub models were finetuned. This will be used to get the category label names from the predicted class ids.\n","\n","Create a visualizer to visualize and plot the results(labels + bounding boxes)."],"id":"7fQ685tYiqDo"},{"cell_type":"code","metadata":{"id":"67054210","executionInfo":{"status":"ok","timestamp":1630483950290,"user_tz":-480,"elapsed":230,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}}},"source":["# Create an id to label name mapping\n","label_map, allowed_class_ids = AvaLabeledVideoFramePaths.read_label_map('ava_action_list.txt')\n","# Create a video visualizer that can plot bounding boxes and visualize actions on bboxes.\n","video_visualizer = VideoVisualizer(81, label_map, top_k=3, mode=\"thres\",thres=0.5)"],"id":"67054210","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBDOibK7iul0"},"source":["#Load an example video"],"id":"CBDOibK7iul0"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4f84a5d","executionInfo":{"status":"ok","timestamp":1630483982457,"user_tz":-480,"elapsed":9523,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"d597ad03-8f1e-4801-d82c-fb3838159254"},"source":["# Load the video\n","video_name = './IMG_2291.MOV'\n","encoded_vid = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_name)\n","print('Completed loading encoded video.')"],"id":"f4f84a5d","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Completed loading encoded video.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a240146","executionInfo":{"status":"ok","timestamp":1629711409237,"user_tz":-480,"elapsed":331,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"bb3e034b-b3bf-419b-f65b-20833efec01b"},"source":["encoded_vid.get_clip(0,1)['video'].shape"],"id":"3a240146","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 31, 540, 960])"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"C-wqgeAZi0Em"},"source":["\n","#Generate bounding boxes and action predictions for all clips in the video."],"id":"C-wqgeAZi0Em"},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"2ee8e924","executionInfo":{"status":"ok","timestamp":1629711479977,"user_tz":-480,"elapsed":70741,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"bf670783-fb22-467f-b73f-3a12c3fe89fa"},"source":["clip_duration = 1.0 # Duration of clip used for each inference step.\n","gif_imgs = []\n","picture_count =0\n","\n","# Video predictions are generated at an internal of 1 sec from 0 seconds to the end seconds in the video.\n","for time_stamp in range(int(encoded_vid.duration)):  # time stamps in video for which clip is sampled.   \n","    print(\"Generating predictions for time stamp: {} sec\".format(time_stamp))\n","    \n","    # Generate clip around the designated time stamps\n","    inp_imgs = encoded_vid.get_clip(\n","        time_stamp - clip_duration/2.0, # start second\n","        time_stamp + clip_duration/2.0  # end second\n","    )\n","    inp_imgs = inp_imgs['video']\n","    \n","    # Generate people bbox predictions using Detectron2's off the self pre-trained predictor\n","    # We use the the middle image in each clip to generate the bounding boxes.\n","    inp_img = inp_imgs[:,inp_imgs.shape[1]//2,:,:]\n","    inp_img = inp_img.permute(1,2,0)\n","    \n","    # Predicted boxes are of the form List[(x_1, y_1, x_2, y_2)]\n","    predicted_boxes = get_person_bboxes(inp_img, predictor) \n","    if len(predicted_boxes) == 0: \n","        print(\"Skipping clip no frames detected at time stamp: \", time_stamp)\n","        continue\n","        \n","    # Preprocess clip and bounding boxes for video action recognition.\n","    inputs, inp_boxes, _ = ava_inference_transform(inp_imgs, predicted_boxes.numpy())\n","    # Prepend data sample id for each bounding box. \n","    # For more details refere to the RoIAlign in Detectron2\n","    inp_boxes = torch.cat([torch.zeros(inp_boxes.shape[0],1), inp_boxes], dim=1)\n","    \n","    # Generate actions predictions for the bounding boxes in the clip.\n","    # The model here takes in the pre-processed video clip and the detected bounding boxes.\n","    if isinstance(inputs, list):\n","        inputs = [inp.unsqueeze(0).to(device) for inp in inputs]\n","    else:\n","        inputs = inputs.unsqueeze(0).to(device)\n","    preds = video_model(inputs, inp_boxes.to(device))\n","\n","    preds= preds.to('cpu')\n","    # The model is trained on AVA and AVA labels are 1 indexed so, prepend 0 to convert to 0 index.\n","    preds = torch.cat([torch.zeros(preds.shape[0],1), preds], dim=1)\n","    \n","    # Plot predictions on the video and save for later visualization.\n","    inp_imgs = inp_imgs.permute(1,2,3,0)\n","    inp_imgs = inp_imgs/255.0\n","    out_img_pred = video_visualizer.draw_clip_range(inp_imgs, preds, predicted_boxes)\n","    #gif_imgs += out_img_pred\n","    \n","    out_img_pred = np.array(out_img_pred)\n","    for index in range(out_img_pred.shape[0]):\n","        img = out_img_pred[index]\n","        img = (255*img).astype(np.uint8)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        cv2.imwrite('./frames/output_%s.jpg'%(picture_count), img)\n","        picture_count += 1\n","print(\"Finished generating predictions.\")"],"id":"2ee8e924","execution_count":null,"outputs":[{"output_type":"stream","text":["Generating predictions for time stamp: 0 sec\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/detectron2/structures/boxes.py:244: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n","  b = self.tensor[item]\n"],"name":"stderr"},{"output_type":"stream","text":["Generating predictions for time stamp: 1 sec\n","Generating predictions for time stamp: 2 sec\n","Generating predictions for time stamp: 3 sec\n","Generating predictions for time stamp: 4 sec\n","Generating predictions for time stamp: 5 sec\n","Generating predictions for time stamp: 6 sec\n","Generating predictions for time stamp: 7 sec\n","Generating predictions for time stamp: 8 sec\n","Generating predictions for time stamp: 9 sec\n","Generating predictions for time stamp: 10 sec\n","Generating predictions for time stamp: 11 sec\n","Generating predictions for time stamp: 12 sec\n","Generating predictions for time stamp: 13 sec\n","Generating predictions for time stamp: 14 sec\n","Finished generating predictions.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HaQ4ootPjH6w"},"source":["#Save predictions as video¶\n","The generated video consists of bounding boxes with predicted actions for each bounding box."],"id":"HaQ4ootPjH6w"},{"cell_type":"code","metadata":{"id":"42f4974d","colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"status":"error","timestamp":1629767466042,"user_tz":-480,"elapsed":254,"user":{"displayName":"kaku chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1x3uosRunSwmaLYbc0hDDSL-THZpmscxuLuJtSw=s64","userId":"01604899390866045422"}},"outputId":"48e3579c-f66e-44ec-e321-d989b5a8be04"},"source":["img = cv2.imread('./frames/output_0.jpg')  # 读取保存的任意一张图片\n","size = (img.shape[1],img.shape[0])  #获取视频中图片宽高度信息\n","\n","cap = cv2.VideoCapture(video_name)\n","total_frames = int(cap.get(7))\n","vid = imageio.get_reader(video_name, 'ffmpeg')\n","fps = vid.get_meta_data()['fps']\n","fourcc = cv2.VideoWriter_fourcc(*\"DIVX\") # 视频编码格式\n","videoWrite = cv2.VideoWriter('./%s_modellargeoutput.mp4'%(video_name),fourcc,fps,size)# 根据图片的大小，创建写入对象 （文件名，支持的编码器，帧率，视频大小（图片大小））\n","\n","for i in range(0, total_frames):\n","    fileName = \"./frames/output_\"+str(i)+'.jpg'    #循环读取所有的图片,假设以数字顺序命名\n","    img = cv2.imread(fileName)\n","    videoWrite.write(img)# 将图片写入所创建的视频对象\n","videoWrite.release()"],"id":"42f4974d","execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-03cdad31af1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ffmpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_meta_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfourcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoWriter_fourcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\"DIVX\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 视频编码格式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Create request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Get format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: No such file: '/content/gdrive/My Drive/slowfast/1. windier 操作正確.mp4'"]}]},{"cell_type":"code","metadata":{"id":"733feb40"},"source":[""],"id":"733feb40","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"22cff6a7"},"source":[""],"id":"22cff6a7","execution_count":null,"outputs":[]}]}